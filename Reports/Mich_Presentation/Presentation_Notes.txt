1.

Hello everyone! I'm Eva and I'm here today to talk about my project: {***}.

First question - what exactly is Mixed Reality (MR)?
And the answer to that is...

2.

...{***}. So just as you can see in the clip from the Iron Man - Tony Stark in the real world is holding a virtual cube as if it was a real world object.

3.

Now what are the ways of interacting with these virtual objects. We don't have a mouse, a keyboard or a touchscreen - the things that we are used to already. What other options do we have?

Well, first of all, we have GAZE which currently is the ray going from in between your eyes.

Then we have GESTURES - tap to click, pinch to zoom, drag to move etc...

CONTROLLER is another option. Something similar to Oculus Rift or HTC Vive controller could be used for Mixed Reality. Or something completely different like Hololens Clicker.

And finally we have VOICE which is playing increasingly important role in the tech world.

Most of the Mixed Reality headsets incorporate all of them to some extent. However, my focus for this project is the combination of gaze and gestures only.

4.

So there are three challenges around the gesture and gaze interactions.

First is the inaccuracy of the sensors. Together with the small movements of the hands and the head they add more noise to the measurement which in turn increases the uncertainty in the intention of the user compared to, let's say, mechanical keyboard.

Second, there's the fact that interaction depends on the environment or the context of the user. So the user can be in an office, on the field fixing something or at home and in all theses places the user interaction with the MR headset will be very different.

And the last challenge is to decide what physical gestures should correspond to what digital action. It's easy to think of a random gesture, but it's not easy to find a gesture that is intuitive and easy to use.

5.

So, following that, a solution neutral problem statement for my project is to {read slide}.

6.

And this problem statement leads to the following objectives:
- {read}
- {read}
- {read}
- {read}

7.

And here's a more specific plan on how the system will be verified and possibly validated.

To make sure that the system satisfies the requirements, automated unit tests will be written, manual unit and integration tests will be performed, and gesture recognition model testing will be done.

To validate the system, if time allows, it will be given to users and feedback will be collected.

8.

The timeline of the project is planned to look like this. Three main sections - text editor component, gestures and user experience optimisation. Each section is followed by testing. And in the end a longer testing time is assigned for possible validation of the system. The greyed out area shows where in the timeline I currently am. And now we can explore in more detail the progress of the project

9.

Here is the basic design of the text editor component. The text editor was written from scratch because none of the built editors were flexible enough. The design patter used is the model-view-presenter. The model (which in this case is the text) and the view never communicate directly. They do that through the middle man - the presenter. When an even is detected by the view (e.g. a tap gesture) it calls the corresponding function in the presenter. That function contains all the logic of things that should happen for the specific event. Corresponding to the logic and the state of the model it initiates updates for both the model and the view.

10.

And here is the text editor in action. It was first built with mouse and keyboard controls to separate the testing of the component and the gesture recognition. We can enter and delete text, we can select text, delete selected text, undo, redo copy and paste. It was built in Unity game engine using C#

11.

Next step was to test the component. Unit tests were written using the NUnit framework and they are run via the Unity Test Runner. These tests allow a quick check to see if the system is still working as expected and they should be run at least once a week. Further testing has been described in the manual testcases. It as a document listing all the testcases that should be done manually after a significant code change, each one with detailed steps and expected results.

12.

The last and the most fun thing that has been done so far is the implementation of one and two finger interactions using Leap Motion sensors and the provided Unity tools. As you can see single finger can tap on the plane and select where the cursor should be placed. And using two fingers, the text can be selected. These are a couple of gestures that I have chosen to implement while experimenting with Leap motion. But as you can see the accuracy is not great.
So the next step in the project is to research what gestures could be better, what implementations could improve the user interface.

But as for now, this is what I have done. Thank you for listening, and does anybody have any questions while I'm letting you enjoy the beautiful Leap Motion Rigged Hands?
